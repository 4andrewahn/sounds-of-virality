{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection\n",
    "Before we begin to explore what aspects of a song contribute to its potential for virality, the audio data must first be scraped from Spotify's API.\n",
    "\n",
    "### `Session` API Client Description\n",
    "To access Spotify's data efficiently and within their guidelines, I set up a special `Session` object. This object contains an access token required to interface with Spotify's end points, as well as logic to auto-renew old access tokens when they expire. Since Spotify has rules on how much information we can ask for at one time our `Session` object is smart about how it collects data. It makes a series of smaller requests instead of one big one, patiently waiting for 1 second after each request. This way, we avoid reaching our rate limit and our per-request fetch limit. In the end, all the pieces of data from these separate requests are put together seamlessly, giving us a complete and comprehensive set of information to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Session class from 'session.py' \n",
    "from session import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a spotify session\n",
    "session = Session()\n",
    "\n",
    "# Generate session access token\n",
    "session.renew_token()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Picking Spotify playlists to explore \n",
    "To ensure a fair comparison between each genre, each playlist selected was curated by Spotify and each have atleast 1 million likes. And though certain tracks may appear in multiple playlists within a genre, our `Session` API Client is designed to prevent duplicate entries. Once all playlists for each group have been processed, we hope to see a roughly even number of tracks across genres. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Define the Spotify IDs for playlists to explore '''\n",
    "\n",
    "# Popular Pop playlists \n",
    "pop_playlist_ids = [\n",
    "    '37i9dQZF1DXcBWIGoYBM5M', # Today's Top Hits - 50 songs\n",
    "    '37i9dQZF1DX0kbJZpiYdZl', # Hot Hits USA - 50 songs\n",
    "    '37i9dQZF1DWUa8ZRTfalHk', # Pop Rising - 85 songs\n",
    "    '37i9dQZEVXbLp5XoPON0wI', # Top Songs USA - 50 songs\n",
    "    '37i9dQZF1DWWvvyNmW9V9a' # tean beats - 104 songs \n",
    "]\n",
    "\n",
    "# Popular Hip-Hop playlists\n",
    "hiphop_playlist_ids = [\n",
    "    '37i9dQZF1DX0XUsuxWHRQd', # RapCaviar - 51 songs\n",
    "    '37i9dQZF1DX6GwdWRQMQpq', # Feelin' Myself - 50 songs\n",
    "    '37i9dQZF1DX2RxBh64BHjQ', # Most Necessary - 100 songs\n",
    "    '37i9dQZF1DWY4xHQp97fN6' # Get Turnt - 100 songs\n",
    "]\n",
    "\n",
    "# Viral internet playlists \n",
    "viral_playlist_ids = [\n",
    "    '37i9dQZF1DX2L0iB23Enbq', # Viral Hits - 76 songs\n",
    "    '37i9dQZF1DX4KeocBrdbJg', # Hits de Internet - 100 songs\n",
    "    '37i9dQZF1DX6OgmB2fwLGd' # Internet People - 100 songs \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Collecting track data from all playlists\n",
    "Before we can request audio-feature data, we need to first collect the Spotify IDs of all tracks. The data returned from `Session` is stored as a dictionary with track IDs as keys and a nested dictonary to hold track attributes and values. By nature of dictionaries, using track IDs as keys prevents duplicates and allows easy data aggregation of audio-feature data later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Collect the basic track data from all playlists '''\n",
    "# Import required library\n",
    "from collections import defaultdict\n",
    "\n",
    "# Collect Pop playlists' track data  \n",
    "pop_data = defaultdict(dict) # Key = Track ID, Val = {'att': 'val', ...}\n",
    "for p_playlist_id in pop_playlist_ids:\n",
    "    # Retrieve playlist from Session\n",
    "    p_tracks = session.get_playlist_tracks(p_playlist_id)\n",
    "\n",
    "    # Add tracks to group dictionary \n",
    "    for pt in p_tracks:\n",
    "        pop_data[pt['id']]['name'] = pt['name']\n",
    "        pop_data[pt['id']]['artist'] = pt['artist']\n",
    "\n",
    "# Collect Hip-Hop playlists' track data  \n",
    "hiphop_data = defaultdict(dict) # Key = Track ID, Val = {'att': 'val', ...}\n",
    "for h_playlist_id in hiphop_playlist_ids:\n",
    "    # Retrieve playlist from Session\n",
    "    h_tracks = session.get_playlist_tracks(h_playlist_id)\n",
    "\n",
    "    # Add tracks to group dictionary \n",
    "    for ht in h_tracks:\n",
    "        hiphop_data[ht['id']]['name'] = ht['name']\n",
    "        hiphop_data[ht['id']]['artist'] = ht['artist']\n",
    "\n",
    "# Collect Viral playlists' track data \n",
    "viral_data = defaultdict(dict) # Key = Track ID, Val = {'att': 'val', ...}\n",
    "for v_playlist_id in viral_playlist_ids:\n",
    "    # Retrieve playlist from Session\n",
    "    v_tracks = session.get_playlist_tracks(v_playlist_id)\n",
    "\n",
    "    # Add tracks to group dictionary\n",
    "    for vt in v_tracks:\n",
    "        viral_data[vt['id']]['name'] = vt['name']\n",
    "        viral_data[vt['id']]['artist'] = vt['artist']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total track recorded: 726\n",
      "Pop: 233 tracks, 32.09% of total dataset\n",
      "Hip-Hop: 241 tracks, 33.2% of total dataset\n",
      "Viral: 252 tracks, 34.71% of total dataset\n"
     ]
    }
   ],
   "source": [
    "''' Check the data samples we collected for each group '''\n",
    "num_pop = len(pop_data)\n",
    "num_hiphop = len(hiphop_data)\n",
    "num_viral = len(viral_data)\n",
    "total = num_pop + num_hiphop + num_viral\n",
    "\n",
    "print(f'Total track recorded: {total}')\n",
    "print(f'Pop: {num_pop} tracks, {round(((num_pop / total) * 100), 2)}% of total dataset')\n",
    "print(f'Hip-Hop: {num_hiphop} tracks, {round(((num_hiphop / total) * 100), 2)}% of total dataset')\n",
    "print(f'Viral: {num_viral} tracks, {round(((num_viral / total) * 100), 2)}% of total dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of the total number of tracks collected, each genre contains roughly the same number of tracks which is what we are looking for. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Collecting Audio-feature data from all playlists\n",
    "Now that the keys for a genre's track data contains its track ID, our `Session` client can request the audio-feature data every track ID in a genre. The returned audio-feature data is formatted as a list of dictionaries with 13 pre-defined attributes as keys which hold a track's audio-features. Every entry in this returned list is appended to the attribute dictionary associated with its track ID. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Collect Audio-feature data for all genres '''\n",
    "# Collect Pop tracks' audio data  \n",
    "pop_track_ids = list(pop_data.keys())\n",
    "pop_audio_features = session.get_audio_features(pop_track_ids)\n",
    "for p_entry in pop_audio_features:\n",
    "    for attr in ATTRIBUTES:\n",
    "        pop_data[p_entry['id']][attr] = p_entry[attr]\n",
    "\n",
    "# Collect Hip-Hop tracks' audio data \n",
    "hiphop_track_ids = list(hiphop_data.keys())\n",
    "hiphop_audio_features = session.get_audio_features(hiphop_track_ids)\n",
    "for h_entry in hiphop_audio_features:\n",
    "    for attr in ATTRIBUTES:\n",
    "        hiphop_data[h_entry['id']][attr] = h_entry[attr]\n",
    "\n",
    "# Collect Viral tracks' audio data \n",
    "viral_track_ids = list(viral_data.keys())\n",
    "viral_audio_features = session.get_audio_features(viral_track_ids)\n",
    "for v_entry in viral_audio_features:\n",
    "    for attr in ATTRIBUTES:\n",
    "        viral_data[v_entry['id']][attr] = v_entry[attr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Data Validation\n",
    "Despite the robust documentation and maintenance of Spotify's API endpoints, we occasionally encounter null entries or records that lack certain attributes. To maintain the integrity of our dataset, it's crucial to perform checks for missing data. This step ensures that the data collection aligns with our expectations and that our analyses will be based on complete and accurate information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Define helper function to validate the collected data '''\n",
    "# The expected length for values in our data dictionaries is 15: ['name', 'artist', and 13 ATTRIBUTES]\n",
    "def show_missing_data(data):\n",
    "    num_entries = len(data)\n",
    "    num_rows_with_missing = 0\n",
    "    missing_attr_ct = {} # Dictionary to count entries with null data for an attribute \n",
    "\n",
    "    # Define all expected keys to find in nested data dictionary for each track \n",
    "    expected_keys = ATTRIBUTES.copy()\n",
    "    expected_keys.extend(['name', 'artist'])\n",
    "    \n",
    "    # Count missing data \n",
    "    for entry in data.values():\n",
    "        is_row_missing_any = False\n",
    "\n",
    "        # Check if key in current row is null\n",
    "        for key in expected_keys: \n",
    "            if key not in entry:\n",
    "                # Row is missing value for key \n",
    "                is_row_missing_any = True\n",
    "                missing_attr_ct[key] = missing_attr_ct.get(key, 0) + 1\n",
    "\n",
    "        if is_row_missing_any:\n",
    "            num_rows_with_missing += 1\n",
    "\n",
    "    # Print results\n",
    "    print(f'Dataset contains {num_entries} rows')\n",
    "    print(f'# of rows with missing values: {num_rows_with_missing}')\n",
    "    missing_attr_items = missing_attr_ct.items()\n",
    "    total_missing = 0\n",
    "    if missing_attr_items: \n",
    "        # Dataset contains attributes with null entries \n",
    "        print('{Missing Attributes : # of null entries}')\n",
    "        \n",
    "        for k, v in missing_attr_items:\n",
    "            print(f'\\'{k}\\' : {v}')\n",
    "            total_missing += v\n",
    "    print(f'# of missing values: {total_missing}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Pop dataset ==\n",
      "Dataset contains 233 rows\n",
      "# of rows with missing values: 0\n",
      "# of missing values: 0\n",
      "\n",
      "== Hip-Hop dataset ==\n",
      "Dataset contains 241 rows\n",
      "# of rows with missing values: 0\n",
      "# of missing values: 0\n",
      "\n",
      "== Viral dataset ==\n",
      "Dataset contains 252 rows\n",
      "# of rows with missing values: 0\n",
      "# of missing values: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "''' Validate our collected datasets '''\n",
    "print('== Pop dataset ==')\n",
    "show_missing_data(pop_data)\n",
    "\n",
    "print('== Hip-Hop dataset ==')\n",
    "show_missing_data(hiphop_data)\n",
    "\n",
    "print('== Viral dataset ==')\n",
    "show_missing_data(viral_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Saving the collected data \n",
    "The dataset collected for each genre contains no missing values and aligns with our expectations of data collection. We can now merge and record each dataset as a csv file with the attribute 'genre' to record a track's genre. The genres are encoded as such: \n",
    "\n",
    "| **Genre** | **Encoding** |\n",
    "| -------- | ------- |\n",
    "| Pop | 0 |\n",
    "| Hip-Hop | 1 |\n",
    "| Viral | 2 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Save track data to csv '''\n",
    "# Import library \n",
    "import csv \n",
    "\n",
    "# Define headers \n",
    "csv_headers = ATTRIBUTES.copy()\n",
    "csv_headers.extend(['name', 'artist', 'genre'])\n",
    "\n",
    "# Write to CSV file\n",
    "with open('spotify_api_data.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=csv_headers)\n",
    "    writer.writeheader()  # Write the header\n",
    "\n",
    "    # Write Pop genre data \n",
    "    for row in pop_data.values():\n",
    "        row['genre'] = 0 # Pop tracks are encoded with 'genre' = 0\n",
    "        writer.writerow(row)\n",
    "\n",
    "    # Write Hip-Hop genre data \n",
    "    for row in hiphop_data.values():\n",
    "        row['genre'] = 1 # Hip-Hop tracks are encoded with 'genre' = 1\n",
    "        writer.writerow(row)\n",
    "\n",
    "    # Write Viral genre data \n",
    "    for row in viral_data.values():\n",
    "        row['genre'] = 2 # Viral tracks are encoded with 'genre' = 2\n",
    "        writer.writerow(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
